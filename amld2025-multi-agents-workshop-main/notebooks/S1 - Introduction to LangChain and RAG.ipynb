{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain and RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the first part of our workshop! In this session, we'll explore how to build AI-powered applications using **LangChain**, a popular framework for developing applications with Large Language Models (LLMs). We'll start with a simple chatbot and then enhance it with Retrieval Augmented Generation (RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Our Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set up our environment. We'll use OpenAI's models, so we need an API key. You can define your `OPENAI_API_KEY` in the `.env` file.\n",
    "\n",
    "The code retrieve the key and sets some global configurations:\n",
    "- `LLM_MODEL`: The specific model we'll use\n",
    "- `LLM_TEMPERATURE`: Controls randomness in responses (0 means very deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY environment variable\")\n",
    "\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "LLM_TEMPERATURE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with creating a basic chatbot using **LangChain**. We'll use:\n",
    "- `ChatOpenAI`: The interface to OpenAI's chat models\n",
    "- `SystemMessage`: Defines the bot's behavior and role\n",
    "- `HumanMessage`: Represents user input\n",
    "\n",
    "Our chatbot will act as a Financial Analyst. We'll create it by:\n",
    "1. Instantiating the model\n",
    "2. Defining a system prompt that sets the bot's role\n",
    "3. Sending a user query and getting a response with `.invoke()`\n",
    "\n",
    "This demonstrates the basic pattern of LLM interactions: prompt â†’ response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a ChatOpenAI instance with the LLM model and temperature\n",
    "base_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = \"\"\"\n",
    "You are a Financial Analyst. Do your best to help the client with their request based on your expertise. Give a succinct and clear response.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request from the client\n",
    "request = \"I want to invest in the technology sector. Can you please define an investment strategy?\"\n",
    "\n",
    "# Message list for the base model\n",
    "messages = [\n",
    "    SystemMessage(BASE_PROMPT),\n",
    "    HumanMessage(request),\n",
    "]\n",
    "\n",
    "# Invoke the model with the messages\n",
    "response = base_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the exciting part! RAG is a technique that enhances LLM responses by giving them access to external knowledge. Instead of relying solely on the model's training data, we can provide relevant information from our own database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement RAG, we need:\n",
    "1. A collection of documents (in our case, a currated set of 1'000 articles from Bloomberg financial news)\n",
    "2. A way to search these documents efficiently (vector database)\n",
    "3. A function to retrieve relevant information based on queries\n",
    "\n",
    "Here we use:\n",
    "- `Chroma`: A vector database for storing and retrieving documents\n",
    "- `OpenAIEmbeddings`: Converts text into vector representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first set up the global configuration for our retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "RETRIEVAL_K = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then define helper functions to load our documents and store them in a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(pickle_filepath: str) -> list[Document]:\n",
    "    \"\"\"Load documents from a pickle file.\"\"\"\n",
    "    with open(pickle_filepath, \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "def initialize_vector_store(document_chunks: list[Document]) -> Chroma:\n",
    "    \"\"\"Reset the Chroma collection and initialize a vector store using document chunks.\"\"\"\n",
    "    Chroma().reset_collection()\n",
    "    embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "    return Chroma.from_documents(documents=document_chunks, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our documents and inspect the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "data_file = \"bloomberg_financial_news_1k.pkl\"\n",
    "\n",
    "# Load the documents from the pickle file\n",
    "documents = load_documents(os.path.join(data_dir, data_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_str = f\"{documents[0].metadata['Headline']}\\n\\n{documents[0].page_content}\"\n",
    "Markdown(doc_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Vector Store and the Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector store and retriever are key components of our RAG system. Here's what happens in this section:\n",
    "\n",
    "- Initialize a new Chroma vector store with these documents\n",
    "- Create a retriever that will fetch the `RETRIEVAL_K` most relevant documents according to their embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vector store with the documents\n",
    "vector_store = initialize_vector_store(documents)\n",
    "\n",
    "# Create a retriever instance from the vector store\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": RETRIEVAL_K})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retriever acts like a smart search engine - when given a question or topic, it returns the most relevant documents from our database. It does so by finding the documents similar embeddings to the query. In LangChain, this is also done with `.invoke()`. Let's try an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_query = \"tech sector market trends\"\n",
    "\n",
    "# TODO: Invoke the retriever with the retrieval query\n",
    "retrieved_documents = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(retrieved_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now augment our basic chatbot by providing it access to the retriever using **LangChain** tools, which allow the model to:\n",
    "- Query the document database if needed\n",
    "- Provide an answer based on the retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a tool with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a tool using the `@tool` decorator from **LangChain** and provide it to the model using `.bind_tools()`. The model will receive all the relevant information about the tool thanks to the decorator. This way it knows how it works and can decide when to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieval(retrieval_query: str) -> list[Document]:\n",
    "    \"\"\"Retrieve documents based on a query.\"\"\"\n",
    "    # TODO: Return documents from the retriever\n",
    "    return ...\n",
    "\n",
    "\n",
    "# Create a list of tools and a dictionnary of tool functions by name\n",
    "tools = [retrieval]\n",
    "tools_by_name = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT = \"\"\"\n",
    "You are a Financial Analyst with access to a Bloomberg Financial News database.\n",
    "\n",
    "Query the database to help the client with their request. Give a succinct and clear response based on the information you find.\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Create the RAG model by binding the base model with the retrieval tool\n",
    "rag_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"I want to invest in the technology sector. Can you please define an investment strategy?\"\n",
    "\n",
    "# TODO: Message list for the RAG model\n",
    "messages = ...\n",
    "\n",
    "# TODO: Invoke the RAG model with the messages\n",
    "rag_response = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the answer. As we can see its content is empty, but a tool call has been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(f\"Content: {rag_response.content}\\n\\nTool Calls: {rag_response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the retrieval tool to retrieve documents following the model's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the RAG model response contains tool calls\n",
    "if rag_response.tool_calls:\n",
    "    # Get the first tool call from the response\n",
    "    tool_call = rag_response.tool_calls[0]\n",
    "\n",
    "    # Get the tool from the tool call\n",
    "    tool = tools_by_name[tool_call[\"name\"]]\n",
    "\n",
    "    # Invoke the tool with the tool call arguments\n",
    "    documents = tool.invoke(tool_call[\"args\"])\n",
    "\n",
    "    # Combine the retrieved documents into a single string\n",
    "    documents_str = \"\\n\\n\".join(\n",
    "        [f\"{doc.metadata['Headline']}\\n\\n{doc.page_content}\\n\" for doc in documents]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(documents_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add the tool's output to the message chain with `ToolMessage`, so the model can answer based on the retrieved documents.\n",
    "\n",
    "*Note: Here we use the base model instead of the RAG model to limit our agent to one retrieval call. A fully autonomous agent could decide to make subsequent calls to best answer the request*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Message list with the retrieved documents for the base model\n",
    "# - RAG System prompt\n",
    "# - Client request\n",
    "# - RAG model response\n",
    "# - Retrieval tool response\n",
    "messages = [\n",
    "    ...,\n",
    "    ...,\n",
    "    ...,\n",
    "    ToolMessage(..., tool_call_id=tool_call[\"id\"]),\n",
    "]\n",
    "\n",
    "# TODO: Invoke the base model with the messages\n",
    "response = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Watch the temperature setting: Lower values (like 0) are usually better for factual responses\n",
    "- Pay attention to the number of retrieved documents (`RETRIEVAL_K`): More isn't always better\n",
    "- The system prompt is crucial: It sets the context and behavior of your bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just learned how to create a chatbot and augment it with a retrieval tool using **LangChain**, this concludes the first part of our workshop!\n",
    "\n",
    "In the next section, we'll discover **LangGraph** and show how it allows to build sophisticated and flexible LLMs workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
